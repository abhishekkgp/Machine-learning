# -*- coding: utf-8 -*-
"""ML_A2_q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tXoRH1VXffch1wvjt95yfwezyXLvtpjK
"""

# imports
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

data=pd.read_csv('winequality-red.csv', sep=';') #importing the data
for i in range(11): # except quality feature every column included here
   x=data.iloc[:,i]
   x_minmax_scaled=(x-x.min())/(x.max()-x.min()) # Normalization using min-max scaling
   data.iloc[:,i]=x_minmax_scaled

#n=len(data.axes[0]) #to find number of rows in each attribute
#print('Number of rows are: ', n)
n=1599
for i in range(n):
  x=data.iloc[i,11]
  if(x<=6) :
    x=0
  else :
    x=1
  data.iloc[i,11]=x   
  
  

print(data.iloc[0:50,:])



"""# Defining All function here"""

def cost_function(X, y, theta): # to compute cost, cost function is defined
    m = len(y)
    z=(X @ theta) # dot production between input X and params 
    h_theta=1 / (1 + np.exp(-z)) #defining logistic hypothesis function i.e. sigmoid function
    #h_theta = sigmoid(X @ theta) # here '@' is used for dot product or matrix multiplication    
    cost = (1/m)*(((-y).T @ np.log(h_theta))-((1-y).T @ np.log(1-h_theta)))
    return cost


def GD(X, y, params, lr, itr): #to udate parameter gradient descent is defined
    m = len(y) # size of sample
    J_theta = np.zeros((itr,1)) # recording history of cost function over iteration

    for i in range(itr):
        z=(X @ params) # dot production between input X and params 
        h=1 / (1 + np.exp(-z)) #defining logistic hypothesis function i.e. sigmoid function
        params = params - (lr/m) * (X.T @ (h - y)) 
        J_theta[i] = cost_function(X, y, params)

    return (J_theta, params)

"""# NOW FOR CALCULATING PARAMS, COST FUNCTION AND PREDICTED VALUE"""

# initialisation
X=data.iloc[:,0:11]
y=data.iloc[:,11]
m = len(y)

X = np.hstack((np.ones((m,1)),X)) # adding/inserting bias as 1 
y = y[:,np.newaxis]
n = np.size(X,1)
params = np.zeros((n,1))

iterations = 1500
learning_rate = 0.01
# Assigning value to function now
cost_function_initial = cost_function(X, y, params) # initial  value of cost function before start of iteration

print("Initial Cost is: {} \n".format(cost_function_initial))

(J_theta, params_final) = GD(X, y, params, learning_rate, iterations) # to calculater J_theta over iteration and parameter final updated value

print(" Final Updated value of Parameters are: \n", params_final, "\n")

# for plotting graph between Cost function and iteration
plt.figure()

plt.plot(range(iterations),J_theta)
plt.title("Cost vs iteration")
plt.xlabel("Number of Iterations")
plt.ylabel("Cost or J_ theta")
plt.show()

# to calculate accuracy
z=(X @ params) # dot production between input X and params 
h=1 / (1 + np.exp(-z)) #defining logistic hypothesis function i.e. sigmoid function
y_pred = np.round(h) # using threshold vaue 0.5 we just rounding up it
accuracy = float(sum(y_pred == y))/ float(len(y))*100

print('Accuracy without scikit learn modal=',accuracy) # Accuracy without any model impletation



"""# Part 2: using scikit learn package"""

from sklearn.linear_model import LogisticRegression

'''X1=X # taking from above part
y1=y # taking value from above part
clf = LogisticRegression(random_state=0).fit(X1, y1)
y_pred_sklearn= LogisticRegression.predict(X1)'''
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix,accuracy_score
 
x1 = X
y1 = y
x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.33, random_state=1)
logmodel = LogisticRegression()
logmodel.fit(x_train, y_train)
 
y_pred_sklearn = logmodel.predict(x_test)

print('Accuracy from scikit learn model=',accuracy_score(y_test, y_pred_sklearn)*100)

'''accuracy1=clf.score(X, y) * 100
print('Accuracy from scikit learn model=',accuracy1)'''



"""# Part 3: 3 fold Cross validation"""

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

X2=X # taking from above part
y2=y # taking value from above part
#X_train, X_test, y_train, y_test = train_test_split(X2, y2, random_state=6)

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
accuracy_logistic=cross_val_score(logreg, X2, y2, cv=3, scoring='accuracy').mean()
print('Mean accuracy of 3 fold cross validation for logistic regression=',accuracy_logistic*100)

#precision recall for Logistic regresion without sklearn
from sklearn.metrics import precision_recall_fscore_support as score

#predicted = [1,2,3,4,5,1,2,1,1,4,5] 
#y_test = [1,2,3,4,5,1,2,1,1,4,1]

precision, recall, fscore, support = score(y1, y_pred) # y_pred is value from logistic regression classifier

print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))

#precision recall with sklearn classifier
from sklearn.metrics import precision_recall_fscore_support as score


precision, recall, fscore, support = score(y_test, y_pred_sklearn) # y_pred is value from logistic regression classifier

print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))

